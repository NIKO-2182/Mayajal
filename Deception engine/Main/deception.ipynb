{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:13:37.574263Z",
     "iopub.status.busy": "2026-01-07T08:13:37.573657Z",
     "iopub.status.idle": "2026-01-07T08:14:30.455494Z",
     "shell.execute_reply": "2026-01-07T08:14:30.454694Z",
     "shell.execute_reply.started": "2026-01-07T08:13:37.574226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, LoftQConfig\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:14:30.457002Z",
     "iopub.status.busy": "2026-01-07T08:14:30.456660Z",
     "iopub.status.idle": "2026-01-07T08:14:31.390815Z",
     "shell.execute_reply": "2026-01-07T08:14:31.390080Z",
     "shell.execute_reply.started": "2026-01-07T08:14:30.456976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Load base model and tokenizer in 4-bit quantization\n",
    "\n",
    "MODEL_NAME = \"Desired Model Name\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",           \n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:15:37.376688Z",
     "iopub.status.busy": "2026-01-07T08:15:37.375895Z",
     "iopub.status.idle": "2026-01-07T08:18:48.264871Z",
     "shell.execute_reply": "2026-01-07T08:18:48.264208Z",
     "shell.execute_reply.started": "2026-01-07T08:15:37.376651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) LoRA with DoRA init, targeting standard transformer linear layers\n",
    "\n",
    "loftq_config=LoftQConfig(\n",
    "    loftq_bits=4,\n",
    ")\n",
    "target_modules = [\"q_proj\", \n",
    "                  \"k_proj\", \n",
    "                  \"v_proj\", \n",
    "                  \"o_proj\", \n",
    "                  \"up_proj\", \n",
    "                  \"down_proj\"] # (Specify the target modules for LoRA adaptation based on the model architecture)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=124,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"loftq\",\n",
    "    loftq_config=loftq_config,\n",
    ")\n",
    "print(\"Calculating...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:25:05.566427Z",
     "iopub.status.busy": "2026-01-07T08:25:05.565606Z",
     "iopub.status.idle": "2026-01-07T08:25:05.574058Z",
     "shell.execute_reply": "2026-01-07T08:25:05.573337Z",
     "shell.execute_reply.started": "2026-01-07T08:25:05.566399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,365,568 || all params: 1,394,837,504 || trainable%: 3.4675\n"
     ]
    }
   ],
   "source": [
    "# 3) Inject adapters on to the base model\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:25:13.062243Z",
     "iopub.status.busy": "2026-01-07T08:25:13.061951Z",
     "iopub.status.idle": "2026-01-07T08:26:22.956642Z",
     "shell.execute_reply": "2026-01-07T08:26:22.955893Z",
     "shell.execute_reply.started": "2026-01-07T08:25:13.062225Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4) Dataset + collator\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"Your training data file path\",})\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=False)\n",
    "tokenized_ds = dataset.map(tokenize_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:26:47.316453Z",
     "iopub.status.busy": "2026-01-07T08:26:47.315743Z",
     "iopub.status.idle": "2026-01-07T08:27:02.584391Z",
     "shell.execute_reply": "2026-01-07T08:27:02.583608Z",
     "shell.execute_reply.started": "2026-01-07T08:26:47.316425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#5) Use Weights & Biases for experiment tracking\n",
    "\n",
    "api_key = \"Your WANDB API KEY\"\n",
    "wandb.login(key=api_key )\n",
    "wandb.init(\n",
    "    project=\"Your Project name\",     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T08:29:46.531262Z",
     "iopub.status.busy": "2026-01-07T08:29:46.530969Z",
     "iopub.status.idle": "2026-01-07T08:29:46.577012Z",
     "shell.execute_reply": "2026-01-07T08:29:46.576311Z",
     "shell.execute_reply.started": "2026-01-07T08:29:46.531232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6) Training arguments \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Your desired Output Directory\",\n",
    "    per_device_train_batch_size=25,          \n",
    "    gradient_accumulation_steps=12,        \n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,             \n",
    "    fp16=True,                      # (Use Bf16 if your GPU supports it)       \n",
    "    optim=\"paged_adamw_8bit\",       # (can us other optimizers too)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.07,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    #ddp_find_unused_parameters=False, (commented out for single GPU runs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8) Merge adapters and save\n",
    "print(\"Merging DoRA LoRA weights...\")\n",
    "merged_model = trainer.model.merge_and_unload()\n",
    "save_path = \"Your desired/save/path\"\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\" Model and tokenizer saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9004807,
     "sourceId": 14131881,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
